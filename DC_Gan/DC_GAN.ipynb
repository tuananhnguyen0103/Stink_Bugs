{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Kiểm tra nếu có GPU khả dụng, nếu không thì sử dụng CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có 1 GPU(s) được phát hiện:\n",
      "GPU 0: Quadro RTX 4000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Có {num_gpus} GPU(s) được phát hiện:\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"Không có GPU nào được phát hiện.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded and shuffled 7254 images with shape torch.Size([7254, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_and_shuffle_images(folder_path, mean, std, img_shape=(128, 128), device='cpu'):\n",
    "    images = []\n",
    "    \n",
    "    # Định nghĩa các transform để xử lý ảnh\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_shape),  # Resize ảnh về kích thước (128, 128)\n",
    "        transforms.ToTensor(),  # Chuyển đổi ảnh thành tensor và chuẩn hóa về [0, 1]\n",
    "        transforms.Normalize(mean, std)  # Chuẩn hóa về [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Đọc và transform ảnh\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Đảm bảo ảnh có 3 kênh (RGB)\n",
    "        img_tensor = transform(img).to(device)  # Áp dụng các transform lên ảnh và chuyển tensor lên GPU\n",
    "        images.append(img_tensor)\n",
    "    \n",
    "    # Shuffle các tensor ảnh\n",
    "    images_tensor = torch.stack(images)  # Chuyển danh sách ảnh thành tensor với kích thước (N, 3, 128, 128)\n",
    "    shuffled_indices = torch.randperm(images_tensor.size(0), device=device)  # Tạo thứ tự ngẫu nhiên để shuffle trên GPU\n",
    "    shuffled_images_tensor = images_tensor[shuffled_indices]  # Trộn các tensor ảnh\n",
    "    \n",
    "    return shuffled_images_tensor\n",
    "\n",
    "# Thiết lập mean và std để chuẩn hóa ảnh\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Thay đổi đường dẫn tới thư mục chứa ảnh \n",
    "folder_path = rf'D:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\Categories\\Images\\Inputs\\256\\aug\\adult'  \n",
    "img_shape = (256, 256)\n",
    "\n",
    "# Kiểm tra xem có GPU không và thiết lập thiết bị phù hợp\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load và shuffle ảnh trên GPU\n",
    "real_images = load_and_shuffle_images(folder_path, mean, std, img_shape, device=device)\n",
    "\n",
    "# Lưu dữ liệu tensor\n",
    "tensor_path = rf'D:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\Categories\\torch\\adult_aug_real_images_256.pt'\n",
    "torch.save(real_images.cpu(), tensor_path)  # Lưu tensor về CPU để không gặp lỗi khi lưu\n",
    "\n",
    "# Kiểm tra kích thước tensor\n",
    "print(f\"Loaded and shuffled {real_images.size(0)} images with shape {real_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lappro\\AppData\\Local\\Temp\\ipykernel_35812\\1651777266.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  real_images = torch.load(tensor_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7254 images with shape torch.Size([7254, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "tensor_path = rf'D:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\Categories\\torch\\adult_aug_real_images_256.pt'\n",
    "img_shape = (256, 256)\n",
    "# Load dữ liệu tensor từ file\n",
    "real_images = torch.load(tensor_path)\n",
    "\n",
    "# Kiểm tra kích thước của dữ liệu đã load\n",
    "print(f\"Loaded {real_images.size(0)} images with shape {real_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # Fixed channels from ngf * 4 to ngf * 8\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(nz, ngf, ndf, nc, device):\n",
    "    netG = Generator(nz, ngf, nc).to(device)\n",
    "    netD = Discriminator(nc, ndf).to(device)\n",
    "    return netG, netD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|▏         | 1/50 [12:26<10:09:27, 746.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 0 to gan_training_results_single_class\\epoch_0_fake_samples.png\n",
      "Epoch [0/50] Loss_D: 0.0013 Loss_G: 8.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   6%|▌         | 3/50 [37:11<9:42:18, 743.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 2 to gan_training_results_single_class\\epoch_2_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 5/50 [1:01:58<9:17:41, 743.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 4 to gan_training_results_single_class\\epoch_4_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  14%|█▍        | 7/50 [1:26:44<8:52:33, 743.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 6 to gan_training_results_single_class\\epoch_6_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  18%|█▊        | 9/50 [1:51:26<8:27:01, 742.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 8 to gan_training_results_single_class\\epoch_8_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  22%|██▏       | 11/50 [2:16:16<8:03:22, 743.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 10 to gan_training_results_single_class\\epoch_10_fake_samples.png\n",
      "Epoch [10/50] Loss_D: 0.0000 Loss_G: 12.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  26%|██▌       | 13/50 [2:41:03<7:38:28, 743.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 12 to gan_training_results_single_class\\epoch_12_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 15/50 [3:05:51<7:13:56, 743.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 14 to gan_training_results_single_class\\epoch_14_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  34%|███▍      | 17/50 [3:30:39<6:49:04, 743.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 16 to gan_training_results_single_class\\epoch_16_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  38%|███▊      | 19/50 [3:55:33<6:25:22, 745.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 18 to gan_training_results_single_class\\epoch_18_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  42%|████▏     | 21/50 [4:20:33<6:01:29, 747.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 20 to gan_training_results_single_class\\epoch_20_fake_samples.png\n",
      "Epoch [20/50] Loss_D: 0.0000 Loss_G: 13.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  46%|████▌     | 23/50 [4:45:33<5:37:02, 748.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 22 to gan_training_results_single_class\\epoch_22_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 25/50 [5:10:36<5:12:36, 750.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 24 to gan_training_results_single_class\\epoch_24_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  54%|█████▍    | 27/50 [5:35:39<4:47:54, 751.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 26 to gan_training_results_single_class\\epoch_26_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  58%|█████▊    | 29/50 [6:00:39<4:22:37, 750.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 28 to gan_training_results_single_class\\epoch_28_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  62%|██████▏   | 31/50 [6:25:41<3:57:40, 750.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 30 to gan_training_results_single_class\\epoch_30_fake_samples.png\n",
      "Epoch [30/50] Loss_D: 0.0000 Loss_G: 15.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  66%|██████▌   | 33/50 [6:50:39<3:32:22, 749.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 32 to gan_training_results_single_class\\epoch_32_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 35/50 [7:15:25<3:06:36, 746.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 34 to gan_training_results_single_class\\epoch_34_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  74%|███████▍  | 37/50 [7:40:10<2:41:16, 744.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 36 to gan_training_results_single_class\\epoch_36_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  78%|███████▊  | 39/50 [8:04:57<2:16:23, 743.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 38 to gan_training_results_single_class\\epoch_38_fake_samples.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  82%|████████▏ | 41/50 [8:29:43<1:51:31, 743.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator output at epoch 40 to gan_training_results_single_class\\epoch_40_fake_samples.png\n",
      "Epoch [40/50] Loss_D: 0.0000 Loss_G: 17.3720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  84%|████████▍ | 42/50 [8:43:42<1:39:45, 748.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m netG, netD \u001b[38;5;241m=\u001b[39m initialize_models(nz, ngf, ndf, nc, device)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mtrain_dcgan_single_class_from_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 53\u001b[0m, in \u001b[0;36mtrain_dcgan_single_class_from_tensor\u001b[1;34m(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir, batch_size)\u001b[0m\n\u001b[0;32m     50\u001b[0m     optimizerG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Save Losses\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     G_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlossG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     54\u001b[0m     D_losses\u001b[38;5;241m.\u001b[39mappend(lossD_real\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m lossD_fake\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Save images every 5% of epochs\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Define the Generator and Discriminator classes (as shown earlier)\n",
    "\n",
    "def train_dcgan_single_class_from_tensor(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir, batch_size):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    real_images = real_images.to(device)  # Ensure real_images is on the correct device\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "        for i in range(0, real_images.size(0), batch_size):\n",
    "            # Get a batch of real images\n",
    "            real_batch = real_images[i:i + batch_size]\n",
    "\n",
    "            # Update Discriminator\n",
    "            netD.zero_grad()\n",
    "\n",
    "            output = netD(real_batch).view(-1)\n",
    "            label = torch.full(output.size(), 1, dtype=torch.float, device=device)  # Label for real images\n",
    "\n",
    "            lossD_real = criterion(output, label)\n",
    "            lossD_real.backward()\n",
    "\n",
    "            # Generate fake images and update Discriminator with fake images\n",
    "            noise = torch.randn(real_batch.size(0), nz, 1, 1, device=device)\n",
    "            fake_images = netG(noise)\n",
    "            output = netD(fake_images.detach()).view(-1)\n",
    "            label = torch.full(output.size(), 0, dtype=torch.float, device=device)  # Label for fake images\n",
    "\n",
    "            lossD_fake = criterion(output, label)\n",
    "            lossD_fake.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # Update Generator\n",
    "            netG.zero_grad()\n",
    "            label.fill_(1)  # Generator tries to trick Discriminator, so label fake images as real (1)\n",
    "            output = netD(fake_images).view(-1)\n",
    "            lossG = criterion(output, label)\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Save Losses\n",
    "            G_losses.append(lossG.item())\n",
    "            D_losses.append(lossD_real.item() + lossD_fake.item())\n",
    "\n",
    "        # Save images every 5% of epochs\n",
    "        if epoch % (num_epochs // 20) == 0:\n",
    "            with torch.no_grad():\n",
    "                fake_images = netG(noise).detach().cpu()\n",
    "            img_save_path = os.path.join(save_dir, f'epoch_{epoch}_fake_samples.png')\n",
    "            vutils.save_image(fake_images[:10], img_save_path, normalize=True)\n",
    "            print(f'Saved generator output at epoch {epoch} to {img_save_path}')\n",
    "\n",
    "        # Print losses every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}] Loss_D: {D_losses[-1]:.4f} Loss_G: {G_losses[-1]:.4f}')\n",
    "\n",
    "    # Visualize Losses\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses, label=\"G\")\n",
    "    plt.plot(D_losses, label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create folder to save results\n",
    "save_dir = \"gan_training_results_single_class\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "  = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "# Hyperparameters\n",
    "nz = 100  # Size of z latent vector (input to generator)\n",
    "ngf = 64  # Size of feature maps in generator\n",
    "ndf = 64  # Size of feature maps in discriminator\n",
    "nc = 3  # Number of color channels (RGB)\n",
    "image_size = 256\n",
    "batch_size = 32  # Set your batch size here\n",
    "# Initialize models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG, netD = initialize_models(nz, ngf, ndf, nc, device)\n",
    "train_dcgan_single_class_from_tensor(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save results\n",
    "save_dir = \"gan_training_results_single_class\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "# Hyperparameters\n",
    "nz = 100  # Size of z latent vector (input to generator)\n",
    "ngf = 64  # Size of feature maps in generator\n",
    "ndf = 64  # Size of feature maps in discriminator\n",
    "nc = 3  # Number of color channels (RGB)\n",
    "image_size = 256\n",
    "# Initialize models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG, netD = initialize_models(nz, ngf, ndf, nc, device)\n",
    "train_dcgan_single_class_from_tensor(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([169])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model using the real_images tensor\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain_dcgan_single_class_from_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m, in \u001b[0;36mtrain_dcgan_single_class_from_tensor\u001b[1;34m(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir)\u001b[0m\n\u001b[0;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m netD(fake_images\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m label\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Label for fake images\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m lossD_fake \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m lossD_fake\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m optimizerD\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\env_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\env_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\env_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:621\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Download\\Edge\\stinkbug\\Merge\\Labeling\\Stink_bugs\\env_gpu\\lib\\site-packages\\torch\\nn\\functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3166\u001b[0m     )\n\u001b[0;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([169])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create folder to save results\n",
    "save_dir = \"gan_training_results_single_class\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train the model using the real_images tensor\n",
    "train_dcgan_single_class_from_tensor(netG, netD, real_images, nz, num_epochs, lr, beta1, device, save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
